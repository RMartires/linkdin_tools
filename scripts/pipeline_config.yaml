# Pipeline Daemon Configuration

# Global Browser Configuration
browser:
  headless: true  # Set to false to show browser windows (useful for debugging)

# Job Scraper Configuration
scraper:
  # Schedule: Daily at 9:00 AM
  schedule:
    hour: 7
    minute: 10
  max_jobs_per_day: 10
  enabled: true
  # Optional: Delay between multiple search runs (seconds)
  delay_between_runs: 30
  # Multiple search configurations - if provided, will iterate through each
  # If not provided, falls back to JOB_KEYWORDS and JOB_LOCATION env vars
  search_configs:
    # Example configurations (uncomment and modify as needed):
    - title: "Senior software engineer"
      location: "Dubai, United Arab Emirates"
      max_jobs: 10  # Optional: override global max_jobs_per_day
    - title: "AI Engineer"
      location: "Dubai, United Arab Emirates"
      max_jobs: 10
    - title: "Senior software engineer"
      location: "Singapore"
      max_jobs: 10
    - title: "AI Engineer"
      location: "Singapore"
      max_jobs: 10

# Company Enricher Configuration
enricher:
  # Schedule: Every hour at :00 (top of the hour)
  schedule:
    minute: "*/5"
  max_retries: 3
  batch_size: 10
  enabled: true

# Draft Generator Configuration
generator:
  # Schedule: Every hour at :30 (30 minutes past the hour)
  schedule:
    minute: "*/5"
  max_retries: 3
  batch_size: 10
  enabled: true

# Google Sheets Integration
google_sheets:
  enabled: true  # Set to true and configure GOOGLE_SHEETS_SPREADSHEET_ID in .env
  # spreadsheet_id is read from env: GOOGLE_SHEETS_SPREADSHEET_ID

# Daemon Configuration
daemon:
  log_file: "logs/pipeline.log"
  pid_file: ".pipeline.pid"
  log_level: "INFO"
